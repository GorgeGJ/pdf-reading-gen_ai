{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3de7c039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "685f1bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U langchain-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6c2e224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ollama is running.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gorgegao/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Check if Ollama is running\n",
    "import requests\n",
    "\n",
    "def is_ollama_running():\n",
    "    try:\n",
    "        response = requests.get(\"http://localhost:11434\")\n",
    "        if response.status_code == 200:\n",
    "            return True\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "# Example usage\n",
    "if is_ollama_running():\n",
    "    print(\"‚úÖ Ollama is running.\")\n",
    "else:\n",
    "    print(\"‚ùå Ollama is NOT running. Please start it with: `ollama run llama3`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc467ad",
   "metadata": {},
   "source": [
    "# üìì Ask My PDFs ‚Äì Demo Notebook\n",
    "This notebook shows how to:\n",
    "- Embed multiple PDFs\n",
    "- Run retrieval-augmented generation queries\n",
    "- Use local LLMs via Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2984282",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gorgegao/Dropbox/My Mac (GorgeÁöÑMacBook Pro)/Desktop/2025/Job Hunting/Project/read-my-pdf-APP/pdf-reading-gen_ai/rag_utils.py:20: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
      "/Users/gorgegao/Dropbox/My Mac (GorgeÁöÑMacBook Pro)/Desktop/2025/Job Hunting/Project/read-my-pdf-APP/pdf-reading-gen_ai/rag_utils.py:22: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: There is no paper mentioned in the provided context. The text appears to be a repetition of an individual's educational background and coursework from New York University, with no mention of a specific paper or topic.\n"
     ]
    }
   ],
   "source": [
    "from rag_utils import load_and_split_pdfs, embed_documents, query_ollama\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Step 1: Load and embed documents\n",
    "docs = load_and_split_pdfs(\"pdfs\")\n",
    "# vectordb = embed_documents(docs, persist_dir=\"chroma_db\")\n",
    "vectordb = embed_documents(docs)\n",
    "\n",
    "# Step 2: Ask a question\n",
    "retriever = vectordb.as_retriever()\n",
    "query = \"What is the main topic of the paper?\"\n",
    "relevant_docs = retriever.invoke(query)\n",
    "context = \"\\n\".join([doc.page_content for doc in relevant_docs[:3]])\n",
    "\n",
    "# Step 3: Generate answer using Ollama\n",
    "response = query_ollama(context, query)\n",
    "print(\"Answer:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee081608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Based on the provided context, here is a summary of Jie Gao's professional background:\n",
      "\n",
      "* Currently working as a Senior Data Scientist at Walmart in Hoboken, New Jersey (since September 2021)\n",
      "* Has over 5 years of experience in customer and marketing analysis, data integrity, modeling, and visualization\n",
      "* Awarded \"Driven of Result\" under in 2023 Winter, rated exemplary during the 1st year in Walmart\n"
     ]
    }
   ],
   "source": [
    "# Ask a question\n",
    "query = \"Can you summarize their professional background?\"\n",
    "relevant_docs = retriever.invoke(query)\n",
    "context = \"\\n\".join([doc.page_content for doc in relevant_docs[:3]])\n",
    "\n",
    "# Generate answer using Ollama\n",
    "response = query_ollama(context, query)\n",
    "print(\"Answer:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef1bfdd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Master's degree. The candidate has a Master in Econometrics and Quantitative Economics from New York University, completed from September 2018 to May 2020.\n"
     ]
    }
   ],
   "source": [
    "# Ask a question\n",
    "query = \"What is this candidate's education level\"\n",
    "relevant_docs = retriever.invoke(query)\n",
    "context = \"\\n\".join([doc.page_content for doc in relevant_docs[:3]])\n",
    "\n",
    "# Generate answer using Ollama\n",
    "response = query_ollama(context, query)\n",
    "print(\"Answer:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc31856c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The context does not explicitly mention SQL as a skill or tool used by the professional. However, it is mentioned that they are familiar with databases such as Oracle and MySQL, which implies some level of familiarity with SQL. Therefore, it can be inferred that the professional likely has experience with SQL, but it's not explicitly stated in their skills section.\n"
     ]
    }
   ],
   "source": [
    "# Ask a question\n",
    "query = \"Do they have experience with SQL?\"\n",
    "relevant_docs = retriever.invoke(query)\n",
    "context = \"\\n\".join([doc.page_content for doc in relevant_docs[:3]])\n",
    "\n",
    "# Generate answer using Ollama\n",
    "response = query_ollama(context, query)\n",
    "print(\"Answer:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "663120c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Based on the context, it appears that the person has only one year of experience in their Master's program (from September 2018 to May 2020). Therefore, I would answer:\n",
      "\n",
      "A: They have 2 years of experience.\n"
     ]
    }
   ],
   "source": [
    "# Ask a question\n",
    "query = \"How many years of experience do they have?\"\n",
    "relevant_docs = retriever.invoke(query)\n",
    "context = \"\\n\".join([doc.page_content for doc in relevant_docs[:3]])\n",
    "\n",
    "# Generate answer using Ollama\n",
    "response = query_ollama(context, query)\n",
    "print(\"Answer:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1da4ac1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: According to the provided context, this professional's top technical skills are:\n",
      "\n",
      "* Tools:\n",
      "\t+ Database (Oracle, MySQL)\n",
      "\t+ Business Intelligence (Tableau)\n",
      "\t+ Adobe Analytics\n",
      "\t+ Github\n",
      "\t+ GCP\n",
      "* Programming Languages:\n",
      "\t+ R (tidyverse, CausalImpact, BSTS)\n",
      "\t+ Python (numpy, pandas, matplotlib, pymc)\n",
      "\n",
      "Note that these are the technical skills listed in the context, but there may be additional skills or expertise not explicitly mentioned.\n"
     ]
    }
   ],
   "source": [
    "# Ask a question\n",
    "query = \"What are their top technical skills?\"\n",
    "relevant_docs = retriever.invoke(query)\n",
    "context = \"\\n\".join([doc.page_content for doc in relevant_docs[:3]])\n",
    "\n",
    "# Generate answer using Ollama\n",
    "response = query_ollama(context, query)\n",
    "print(\"Answer:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ccea1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
